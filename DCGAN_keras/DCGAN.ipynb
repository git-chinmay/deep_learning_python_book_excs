{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRKggi2LHqMm"
   },
   "source": [
    "## A basic GAN using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00Qo-zjrHzTa"
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Cul0VxymHnmy"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "W3-heWykH8q3"
   },
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aU5Q30TFII4q",
    "outputId": "a6497240-3250-49a9-85c3-8959905fc583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_input = keras.Input(shape=(latent_dim))\n",
    "\n",
    "# Transforms the input into a 16 * 16 * 128 channel feature map\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Upsampling to 32 x 32\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Produces a 32x32 1-channel feature map (shape of a CIFAR 10 image)\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "# Instantiate the generator model, which maps the input of shape(latent_dim) into an image of 32x32x3\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n08cAgiuKvyk"
   },
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyVU_wftISZM",
    "outputId": "9257bed1-36e7-4a66-90ba-34f64fef4768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from keras.optimizers import rmsprop_v2\n",
    "\n",
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# One dropout layer an important trick to introduce some randomness so that GAN will not stuck during training\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "# Classification layer\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Instantiate the discriminator which turns a 32x32x3 input into a binary classification decision(real or fake)\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "\n",
    "#Using Gradient clipping(by value) in optimizer\n",
    "# To stabilize training uses learning rate decay\n",
    "d_optimizer = keras.optimizers.rmsprop_v2.RMSprop(lr=0.008, clipvalue=1.0, decay=1e-8)\n",
    "\n",
    "discriminator.compile(optimizer=d_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG3e3rdoPYwV"
   },
   "source": [
    "### Adversial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xUs6izxJNFgv"
   },
   "outputs": [],
   "source": [
    "# setting the weights to non trainable , this will only apply to the GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = keras.Input(shape=(latent_dim))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "\n",
    "gan_optmizer = keras.optimizers.rmsprop_v2.RMSprop(learning_rate=0.004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optmizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kad_MhEQQhlZ"
   },
   "source": [
    "### Implementing GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdoZeFEWQdZY",
    "outputId": "bcd5f693-ac07-49c2-e6b0-017df7bcc812"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (_ , _) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "VbWchY7sQ51C",
    "outputId": "61ea565f-b913-42f4-e284-e8213893cd2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5089 calls to <function Model.make_train_function.<locals>.train_function at 0x000001C93AADD828> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Discrminator loss: 0.6876260042190552\n",
      "Adversial loss: 5.019658276414702e-15\n",
      "Discrminator loss: 34.291839599609375\n",
      "Adversial loss: 0.0\n",
      "Discrminator loss: 2672.966796875\n",
      "Adversial loss: 8135.3095703125\n",
      "Discrminator loss: 97008.28125\n",
      "Adversial loss: 102641.6640625\n",
      "Discrminator loss: -221.44308471679688\n",
      "Adversial loss: 422450.0625\n",
      "Discrminator loss: -4380.3193359375\n",
      "Adversial loss: 1915486.625\n",
      "Discrminator loss: -27179.90234375\n",
      "Adversial loss: 6485943.0\n",
      "Discrminator loss: -82714.234375\n",
      "Adversial loss: 17741420.0\n",
      "Discrminator loss: -183052.71875\n",
      "Adversial loss: 39165248.0\n",
      "Discrminator loss: -430937.5625\n",
      "Adversial loss: 74572208.0\n",
      "Discrminator loss: -834048.6875\n",
      "Adversial loss: 133645648.0\n",
      "Discrminator loss: -1729014.75\n",
      "Adversial loss: 220806400.0\n",
      "Discrminator loss: -2624334.0\n",
      "Adversial loss: 348074016.0\n",
      "Discrminator loss: -3350828.5\n",
      "Adversial loss: 516779360.0\n",
      "Discrminator loss: -5204601.0\n",
      "Adversial loss: 729455744.0\n",
      "Discrminator loss: -8776949.0\n",
      "Adversial loss: 1001940288.0\n",
      "Discrminator loss: -13611421.0\n",
      "Adversial loss: 1380176512.0\n",
      "Discrminator loss: -17337344.0\n",
      "Adversial loss: 1828228352.0\n",
      "Discrminator loss: -15629267.0\n",
      "Adversial loss: 2376381184.0\n",
      "Discrminator loss: -27820832.0\n",
      "Adversial loss: 3044220928.0\n",
      "Discrminator loss: -33962312.0\n",
      "Adversial loss: 3821350144.0\n",
      "Discrminator loss: -30165324.0\n",
      "Adversial loss: 4722138112.0\n",
      "Discrminator loss: -57916588.0\n",
      "Adversial loss: 5871342080.0\n",
      "Discrminator loss: -73512496.0\n",
      "Adversial loss: 7213773824.0\n",
      "Discrminator loss: -87723608.0\n",
      "Adversial loss: 8765581312.0\n",
      "Discrminator loss: -87935600.0\n",
      "Adversial loss: 10644046848.0\n",
      "Discrminator loss: -114782872.0\n",
      "Adversial loss: 12570291200.0\n",
      "Discrminator loss: -113936304.0\n",
      "Adversial loss: 14719480832.0\n",
      "Discrminator loss: -131712104.0\n",
      "Adversial loss: 17636458496.0\n",
      "Discrminator loss: -155295712.0\n",
      "Adversial loss: 20600231936.0\n",
      "Discrminator loss: -286258240.0\n",
      "Adversial loss: 23822112768.0\n",
      "Discrminator loss: -339129408.0\n",
      "Adversial loss: 28125622272.0\n",
      "Discrminator loss: -305258208.0\n",
      "Adversial loss: 32579512320.0\n",
      "Discrminator loss: -384836704.0\n",
      "Adversial loss: 36418207744.0\n",
      "Discrminator loss: -340556032.0\n",
      "Adversial loss: 41602514944.0\n",
      "Discrminator loss: -581040000.0\n",
      "Adversial loss: 48493617152.0\n",
      "Discrminator loss: -623579840.0\n",
      "Adversial loss: 55302955008.0\n",
      "Discrminator loss: -672811456.0\n",
      "Adversial loss: 62920830976.0\n",
      "Discrminator loss: -714834688.0\n",
      "Adversial loss: 70168748032.0\n",
      "Discrminator loss: -792316544.0\n",
      "Adversial loss: 79323283456.0\n",
      "Discrminator loss: -863574400.0\n",
      "Adversial loss: 87422312448.0\n",
      "Discrminator loss: -1070663168.0\n",
      "Adversial loss: 97643323392.0\n",
      "Discrminator loss: -1030675776.0\n",
      "Adversial loss: 110431019008.0\n",
      "Discrminator loss: -1068904256.0\n",
      "Adversial loss: 123152211968.0\n",
      "Discrminator loss: -1157089024.0\n",
      "Adversial loss: 138895228928.0\n",
      "Discrminator loss: -1684048384.0\n",
      "Adversial loss: 151151542272.0\n",
      "Discrminator loss: -1633094016.0\n",
      "Adversial loss: 169619668992.0\n",
      "Discrminator loss: -1457185792.0\n",
      "Adversial loss: 186368966656.0\n",
      "Discrminator loss: -2427125760.0\n",
      "Adversial loss: 201946955776.0\n",
      "Discrminator loss: -2094003968.0\n",
      "Adversial loss: 224240238592.0\n",
      "Discrminator loss: -3026164992.0\n",
      "Adversial loss: 251388182528.0\n",
      "Discrminator loss: -2834397184.0\n",
      "Adversial loss: 270047477760.0\n",
      "Discrminator loss: -2782605056.0\n",
      "Adversial loss: 297553854464.0\n",
      "Discrminator loss: -4034909440.0\n",
      "Adversial loss: 323383787520.0\n",
      "Discrminator loss: -4010823680.0\n",
      "Adversial loss: 354851389440.0\n",
      "Discrminator loss: -4840053760.0\n",
      "Adversial loss: 394424942592.0\n",
      "Discrminator loss: -4026894848.0\n",
      "Adversial loss: 425159917568.0\n",
      "Discrminator loss: -6241111040.0\n",
      "Adversial loss: 459169333248.0\n",
      "Discrminator loss: -5252593664.0\n",
      "Adversial loss: 503609982976.0\n",
      "Discrminator loss: -5192369152.0\n",
      "Adversial loss: 544402833408.0\n",
      "Discrminator loss: -6006598656.0\n",
      "Adversial loss: 583485554688.0\n",
      "Discrminator loss: -7049158656.0\n",
      "Adversial loss: 632339234816.0\n",
      "Discrminator loss: -7385617920.0\n",
      "Adversial loss: 689119625216.0\n",
      "Discrminator loss: -8592506880.0\n",
      "Adversial loss: 743955890176.0\n",
      "Discrminator loss: -8715495424.0\n",
      "Adversial loss: 796709093376.0\n",
      "Discrminator loss: -6607346688.0\n",
      "Adversial loss: 854390341632.0\n",
      "Discrminator loss: -10028823552.0\n",
      "Adversial loss: 922695106560.0\n",
      "Discrminator loss: -11943884800.0\n",
      "Adversial loss: 984616861696.0\n",
      "Discrminator loss: -12196403200.0\n",
      "Adversial loss: 1069917536256.0\n",
      "Discrminator loss: -12726106112.0\n",
      "Adversial loss: 1135496265728.0\n",
      "Discrminator loss: -12861551616.0\n",
      "Adversial loss: 1212764389376.0\n",
      "Discrminator loss: -15303575552.0\n",
      "Adversial loss: 1283447980032.0\n",
      "Discrminator loss: -15952579584.0\n",
      "Adversial loss: 1413451087872.0\n",
      "Discrminator loss: -17181802496.0\n",
      "Adversial loss: 1479580712960.0\n",
      "Discrminator loss: -19511191552.0\n",
      "Adversial loss: 1556668743680.0\n",
      "Discrminator loss: -18719477760.0\n",
      "Adversial loss: 1692243329024.0\n",
      "Discrminator loss: -23924586496.0\n",
      "Adversial loss: 1834975494144.0\n",
      "Discrminator loss: -23665782784.0\n",
      "Adversial loss: 1909851553792.0\n",
      "Discrminator loss: -22186797056.0\n",
      "Adversial loss: 2010722336768.0\n",
      "Discrminator loss: -26013011968.0\n",
      "Adversial loss: 2154071195648.0\n",
      "Discrminator loss: -31087857664.0\n",
      "Adversial loss: 2296628117504.0\n",
      "Discrminator loss: -31016452096.0\n",
      "Adversial loss: 2421965455360.0\n",
      "Discrminator loss: -28260952064.0\n",
      "Adversial loss: 2602558029824.0\n",
      "Discrminator loss: -23444013056.0\n",
      "Adversial loss: 2695525302272.0\n",
      "Discrminator loss: -22015137792.0\n",
      "Adversial loss: 2869604384768.0\n",
      "Discrminator loss: -29120442368.0\n",
      "Adversial loss: 3085345226752.0\n",
      "Discrminator loss: -35624550400.0\n",
      "Adversial loss: 3235918381056.0\n",
      "Discrminator loss: -43604733952.0\n",
      "Adversial loss: 3460216913920.0\n",
      "Discrminator loss: -46845120512.0\n",
      "Adversial loss: 3662368473088.0\n",
      "Discrminator loss: -37911109632.0\n",
      "Adversial loss: 3801637453824.0\n",
      "Discrminator loss: -46073184256.0\n",
      "Adversial loss: 3956781350912.0\n",
      "Discrminator loss: -59001774080.0\n",
      "Adversial loss: 4194715041792.0\n",
      "Discrminator loss: -47409729536.0\n",
      "Adversial loss: 4460377014272.0\n",
      "Discrminator loss: -32332742656.0\n",
      "Adversial loss: 4653990281216.0\n",
      "Discrminator loss: -58028875776.0\n",
      "Adversial loss: 4905793224704.0\n",
      "Discrminator loss: -57771745280.0\n",
      "Adversial loss: 5238412017664.0\n",
      "Discrminator loss: -65214676992.0\n",
      "Adversial loss: 5435011104768.0\n",
      "Discrminator loss: -69141610496.0\n",
      "Adversial loss: 5701293309952.0\n",
      "Discrminator loss: -58422910976.0\n",
      "Adversial loss: 6002253496320.0\n",
      "Discrminator loss: -66449989632.0\n",
      "Adversial loss: 6398877892608.0\n"
     ]
    }
   ],
   "source": [
    "# Selecting only frog image as we will trin with onely class out of 10\n",
    "x_train = x_train[y_train.flatten() == 6]\n",
    "\n",
    "# Normalizing the data\n",
    "x_train = x_train.reshape((x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
    "\n",
    "iterations = 100\n",
    "batch_size = 20\n",
    "save_dir_gen = './generated_image_folder'\n",
    "save_dir_real = './real_image_folder'\n",
    "\n",
    "# sample random points from latent space\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "  random_latent_vector = np.random.normal(size=(batch_size, latent_dim))\n",
    "\n",
    "  # Decode them to fake image\n",
    "  generated_images = generator.predict(random_latent_vector)\n",
    "\n",
    "  # Combine fake image with original image\n",
    "  stop = start + batch_size\n",
    "  real_images = x_train[start: stop]\n",
    "  combined_images = np.concatenate([generated_images, real_images])\n",
    "\n",
    "  # Assembling labels, discrminating between real and fake\n",
    "  labels = np.concatenate([np.ones((batch_size,1)), np.zeros((batch_size,1))])\n",
    "  # Add random noise to labels\n",
    "  labels = labels + 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "  # Trains the discriminator\n",
    "  d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "  \n",
    "  # samples the random points in the latent space\n",
    "  random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "\n",
    "  # Assembling labels that says these are all real images(lie)\n",
    "  misleading_targets = np.zeros((batch_size, 1))\n",
    "  \n",
    "  # Tains the generator via the GAN model where the discrminitaor weights are frozen\n",
    "  a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "\n",
    "  start = start + batch_size\n",
    "  if start > len(x_train) - batch_size:\n",
    "    start = 0\n",
    "\n",
    "  # Occasionaly saves and plots every 100 epochs\n",
    "  if step % 100 == 0:\n",
    "    gan.save_weights('gan.h5')\n",
    "\n",
    "  # print metrics\n",
    "  print(f\"Discrminator loss: {d_loss}\")\n",
    "  print(f\"Adversial loss: {a_loss}\")\n",
    "\n",
    "  # Save one generated image\n",
    "  img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "  img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n",
    "\n",
    "  # Save one real image for comparison\n",
    "  img = image.array_to_img(real_images[0] * 255, scale=False)\n",
    "  img.save(os.path.join(save_dir_real, 'real_frog' + str(step) + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVprdsZ6WfYq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
